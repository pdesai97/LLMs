# -*- coding: utf-8 -*-
"""Purvesh_Desai_DATA78000_Assg2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GW1yvmVV84rQiHf0INKOyFpnLugkzF6b
"""

import numpy as np
import re

import nltk
# if you haven't downloaded punkt before, you only need to run the line below once
nltk.download('punkt')
from nltk import word_tokenize
from nltk import sent_tokenize

from nltk.util import bigrams
from nltk.lm.preprocessing import padded_everygram_pipeline

import requests

# you will need to leverage the requests package
r = requests.get(r'https://www.gutenberg.org/cache/epub/71661/pg71661.txt')
great_gatsby = r.text

# first, remove unwanted new line and tab characters from the text
for char in ["\n", "\r", "\d", "\t"]:
    great_gatsby = great_gatsby.replace(char, " ")

# check
print(great_gatsby[:100])

# remove the metadata at the beginning - this is slightly different for each book
great_gatsby = great_gatsby[3677:]

# this is simplified for demonstration
def sample_clean_text(text: str):
    # lowercase
    text = text.lower()

    # remove punctuation from text
    text = re.sub(r"[^\w\s]", "", text)

    # tokenize the text
    tokens = nltk.word_tokenize(text)

    # return your tokens
    return tokens

# call the function
sample_tokens = sample_clean_text(text = great_gatsby)

# check
print(sample_tokens[:50])

# create bigrams from the sample tokens
my_bigrams = bigrams(sample_tokens)

# check
list(my_bigrams)[:10]

# 2 is for bigrams
n = 2
#specify the text you want to use
text = great_gatsby

# step 1: tokenize the text into sentences
sentences = nltk.sent_tokenize(text)

# step 2: tokenize each sentence into words
tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]

# step 3: convert each word to lowercase
tokenized_text = [[word.lower() for word in sent] for sent in tokenized_sentences]

#notice the sentence breaks and what the first 10 items of the tokenized text
print(tokenized_text[0])

# notice what the first 10 items are of the vocabulary
print(text[:10])

# we imported this function from nltk
train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)

from nltk.lm import MLE
# we imported this function from nltk linear models (lm)
# it is for Maximum Likelihood Estimation

# MLE is the model we will use
lm = MLE(n)

# currently the vocab length is 0: it has no prior knowledge
len(lm.vocab)

# fit the model
# training data is the bigrams and unigrams
# the vocab is all the sentence tokens in the corpus

lm.fit(train_data, padded_sents)
len(lm.vocab)

# inspect the model's vocabulary.
# be sure that a sentence you know exists (from tokenized_text) is in the
print(lm.vocab.lookup(tokenized_text[0]))

# see what happens when we include a word that is not in the vocab.
print(lm.vocab.lookup('and listening in place of going about your business.'.split()))

# how many times does daisy appear in the model?
print(lm.counts['business'])

# what is the probability of daisy appearing?
# this is technically the relative frequency of daisy appearing
lm.score('business')

# how often does (daisy, and) occur and what is the relative frequency?
print(lm.counts[['business']]['of'])
lm.score('of', 'business'.split())

# what is the score of 'UNK'?

lm.score("<UNK>")

# generate a 20 word sentence starting with the word, 'daisy'

print(lm.generate(20, text_seed= 'business', random_seed=1))

from nltk.tokenize.treebank import TreebankWordDetokenizer

detokenize = TreebankWordDetokenizer().detokenize

def generate_sent(lm, num_words, text_seed, random_seed=42):
    """
    :param model: An ngram language model from `nltk.lm.model`.
    :param num_words: Max no. of words to generate.
    :param random_seed: Seed value for random.
    """
    content = []
    for token in lm.generate(num_words, text_seed=text_seed, random_seed=random_seed):
        if token == '<s>':
            continue
        if token == '</s>':
            break
        content.append(token)
    return detokenize(content)

# Now generate sentences that look much nicer.
generate_sent(lm, 20, text_seed='business', random_seed = 1)